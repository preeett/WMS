{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preeett/WMS/blob/main/StudyBuddy_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x60TAFyCc7XQ",
        "outputId": "b700931f-c0a1-4d40-9d06-378226e78524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit pyngrok google-generativeai sentence-transformers faiss-cpu PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile studybuddy_app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "import textwrap\n",
        "\n",
        "# -------------------------------\n",
        "# Attempt to access the GOOGLE_API_KEY\n",
        "try:\n",
        "    GOOGLE_API_KEY = AIzaSyDWG2jT69zThbwsGxyfg52fmGAQmK7mn70\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google API Key successfully loaded and configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Google API Key: {e}\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is correctly set in Colab secrets.\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "document_text = \"\"\n",
        "chunks = []\n",
        "chunk_embeddings = None\n",
        "index = None\n",
        "chat_history = []\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=300, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def get_relevant_chunks(query, k=3):\n",
        "    query_vec = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vec), k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def ask_studybuddy(query):\n",
        "    if not chunks:\n",
        "        return \"⚠️ Please upload and process a PDF first.\"\n",
        "\n",
        "    relevant_chunks = get_relevant_chunks(query)\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    prompt = f\"\"\"You are StudyBuddy, a helpful academic assistant.\n",
        "\n",
        "Use the context below to answer the user's question.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            answer = response.text.strip()\n",
        "            chat_history.append((query, answer))\n",
        "            return format_chat_history()\n",
        "    except concurrent.futures.TimeoutError:\n",
        "        return \"❌ Gemini timed out.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "def generate_quiz():\n",
        "    if not document_text:\n",
        "        return \"⚠️ Please upload a PDF first.\"\n",
        "\n",
        "    prompt = f\"\"\"Generate 5 quiz questions from the following academic material.\n",
        "Provide a mix of MCQs and short answers.\n",
        "\n",
        "Material:\n",
        "\\\"\\\"\\\"\n",
        "{document_text[:1000]}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Format:\n",
        "Q1. [Question]\n",
        "A. [Option 1]\n",
        "B. [Option 2]\n",
        "C. [Option 3]\n",
        "D. [Option 4]\n",
        "Answer: [Correct Option or Short Answer]\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "def format_chat_history():\n",
        "    return \"\\n\\n\".join([f\"🧑‍🎓 {q}\\n🤖 {textwrap.fill(a, 100)}\" for q, a in chat_history])\n",
        "\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"StudyBuddy\", page_icon=\"📘\")\n",
        "st.title(\"📘 StudyBuddy: Ask Your PDF Anything\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"📂 Upload a PDF\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        pdf_path = tmp.name\n",
        "\n",
        "    with st.spinner(\"Processing PDF...\"):\n",
        "        document_text = extract_text_from_pdf(pdf_path)\n",
        "        chunks = chunk_text(document_text)\n",
        "        chunk_embeddings = embedder.encode(chunks)\n",
        "        index = faiss.IndexFlatL2(chunk_embeddings[0].shape[0])\n",
        "        index.add(np.array(chunk_embeddings))\n",
        "        chat_history.clear()\n",
        "\n",
        "    st.success(\"✅ PDF processed. You can now ask questions or generate quizzes.\")\n",
        "\n",
        "st.header(\"💬 Ask a Question\")\n",
        "query = st.text_input(\"Enter your question about the PDF\")\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            answer = ask_studybuddy(query)\n",
        "            st.text_area(\"📚 Answer\", value=answer, height=250)\n",
        "    else:\n",
        "        st.warning(\"Please enter a valid question.\")\n",
        "\n",
        "st.header(\"📝 Generate Quiz\")\n",
        "if st.button(\"Generate Quiz from PDF\"):\n",
        "    with st.spinner(\"Generating quiz...\"):\n",
        "        quiz = generate_quiz()\n",
        "        st.text_area(\"🧪 Quiz\", value=quiz, height=300)\n",
        "\n",
        "if chat_history:\n",
        "    st.header(\"📖 Chat History\")\n",
        "    st.markdown(format_chat_history())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLq8YYbdEuG",
        "outputId": "0b42aaf3-22c3-4136-f795-779e2f55dfc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing studybuddy_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ],
      "metadata": {
        "id": "nKq8alNRdMyL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Start Streamlit\n",
        "def run():\n",
        "    subprocess.Popen([\"streamlit\", \"run\", \"studybuddy_app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "threading.Thread(target=run).start()\n",
        "time.sleep(10)\n",
        "\n",
        "# Launch Cloudflared tunnel\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hl6adaixd1s8",
        "outputId": "efc22f22-53d3-4c6f-d662-256f1bb61c90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2243031624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Launch Cloudflared tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}