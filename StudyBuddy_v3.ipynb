{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preeett/WMS/blob/main/StudyBuddy_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x60TAFyCc7XQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit pyngrok google-generativeai sentence-transformers faiss-cpu PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile studybuddy_app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "import textwrap\n",
        "\n",
        "# -------------------------------\n",
        "# Attempt to access the GOOGLE_API_KEY\n",
        "try:\n",
        "    GOOGLE_API_KEY = AIzaSyDWG2jT69zThbwsGxyfg52fmGAQmK7mn70\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google API Key successfully loaded and configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Google API Key: {e}\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is correctly set in Colab secrets.\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "document_text = \"\"\n",
        "chunks = []\n",
        "chunk_embeddings = None\n",
        "index = None\n",
        "chat_history = []\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=300, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def get_relevant_chunks(query, k=3):\n",
        "    query_vec = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vec), k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def ask_studybuddy(query):\n",
        "    if not chunks:\n",
        "        return \"‚ö†Ô∏è Please upload and process a PDF first.\"\n",
        "\n",
        "    relevant_chunks = get_relevant_chunks(query)\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    prompt = f\"\"\"You are StudyBuddy, a helpful academic assistant.\n",
        "\n",
        "Use the context below to answer the user's question.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            answer = response.text.strip()\n",
        "            chat_history.append((query, answer))\n",
        "            return format_chat_history()\n",
        "    except concurrent.futures.TimeoutError:\n",
        "        return \"‚ùå Gemini timed out.\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "def generate_quiz():\n",
        "    if not document_text:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF first.\"\n",
        "\n",
        "    prompt = f\"\"\"Generate 5 quiz questions from the following academic material.\n",
        "Provide a mix of MCQs and short answers.\n",
        "\n",
        "Material:\n",
        "\\\"\\\"\\\"\n",
        "{document_text[:1000]}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Format:\n",
        "Q1. [Question]\n",
        "A. [Option 1]\n",
        "B. [Option 2]\n",
        "C. [Option 3]\n",
        "D. [Option 4]\n",
        "Answer: [Correct Option or Short Answer]\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "def format_chat_history():\n",
        "    return \"\\n\\n\".join([f\"üßë‚Äçüéì {q}\\nü§ñ {textwrap.fill(a, 100)}\" for q, a in chat_history])\n",
        "\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"StudyBuddy\", page_icon=\"üìò\")\n",
        "st.title(\"üìò StudyBuddy: Ask Your PDF Anything\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"üìÇ Upload a PDF\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        pdf_path = tmp.name\n",
        "\n",
        "    with st.spinner(\"Processing PDF...\"):\n",
        "        document_text = extract_text_from_pdf(pdf_path)\n",
        "        chunks = chunk_text(document_text)\n",
        "        chunk_embeddings = embedder.encode(chunks)\n",
        "        index = faiss.IndexFlatL2(chunk_embeddings[0].shape[0])\n",
        "        index.add(np.array(chunk_embeddings))\n",
        "        chat_history.clear()\n",
        "\n",
        "    st.success(\"‚úÖ PDF processed. You can now ask questions or generate quizzes.\")\n",
        "\n",
        "st.header(\"üí¨ Ask a Question\")\n",
        "query = st.text_input(\"Enter your question about the PDF\")\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            answer = ask_studybuddy(query)\n",
        "            st.text_area(\"üìö Answer\", value=answer, height=250)\n",
        "    else:\n",
        "        st.warning(\"Please enter a valid question.\")\n",
        "\n",
        "st.header(\"üìù Generate Quiz\")\n",
        "if st.button(\"Generate Quiz from PDF\"):\n",
        "    with st.spinner(\"Generating quiz...\"):\n",
        "        quiz = generate_quiz()\n",
        "        st.text_area(\"üß™ Quiz\", value=quiz, height=300)\n",
        "\n",
        "if chat_history:\n",
        "    st.header(\"üìñ Chat History\")\n",
        "    st.markdown(format_chat_history())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLq8YYbdEuG",
        "outputId": "d2cbbf30-6b1f-4f1d-a113-e56c78c5bc43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting studybuddy_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ],
      "metadata": {
        "id": "nKq8alNRdMyL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Start Streamlit\n",
        "def run():\n",
        "    subprocess.Popen([\"streamlit\", \"run\", \"studybuddy_app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "threading.Thread(target=run).start()\n",
        "time.sleep(10)\n",
        "\n",
        "# Launch Cloudflared tunnel\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl6adaixd1s8",
        "outputId": "a031ee0d-7c73-4448-8f55-a53826b2874c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-07-13T06:39:24Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-07-13T06:39:24Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m |  https://part-acrobat-coupon-holdings.trycloudflare.com                                    |\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 302064be-6032-470b-b8aa-a35901f9ef58\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "2025/07/13 06:39:28 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-07-13T06:39:28Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mb2f87b85-d2b3-4532-9da8-fb452ff1de75 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}